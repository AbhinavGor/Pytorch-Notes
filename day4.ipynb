{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872b5e44",
   "metadata": {},
   "source": [
    "# MNIST + MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8128dd4",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727b5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa00727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920965f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_loader)\n",
    "feat, labels = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c97803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(feat.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74104f15",
   "metadata": {},
   "source": [
    "## 2. Build an MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6dea2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae0bc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec47b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 2.313735246658325\n",
      "Epoch: 2 | Loss: 2.254995346069336\n",
      "Epoch: 3 | Loss: 2.216642379760742\n",
      "Epoch: 4 | Loss: 2.2066233158111572\n",
      "Epoch: 5 | Loss: 2.116137981414795\n",
      "Accuracy=0.40625\n",
      "Epoch: 6 | Loss: 2.1082653999328613\n",
      "Epoch: 7 | Loss: 2.0303168296813965\n",
      "Epoch: 8 | Loss: 1.9880492687225342\n",
      "Epoch: 9 | Loss: 1.9071979522705078\n",
      "Epoch: 10 | Loss: 1.9142886400222778\n",
      "Accuracy=0.6015625\n",
      "Epoch: 11 | Loss: 1.8285441398620605\n",
      "Epoch: 12 | Loss: 1.789801001548767\n",
      "Epoch: 13 | Loss: 1.7015624046325684\n",
      "Epoch: 14 | Loss: 1.6269309520721436\n",
      "Epoch: 15 | Loss: 1.6034806966781616\n",
      "Accuracy=0.671875\n",
      "Epoch: 16 | Loss: 1.471725583076477\n",
      "Epoch: 17 | Loss: 1.4991382360458374\n",
      "Epoch: 18 | Loss: 1.4325751066207886\n",
      "Epoch: 19 | Loss: 1.3637492656707764\n",
      "Epoch: 20 | Loss: 1.2786868810653687\n",
      "Accuracy=0.7734375\n",
      "Epoch: 21 | Loss: 1.1804617643356323\n",
      "Epoch: 22 | Loss: 1.1847728490829468\n",
      "Epoch: 23 | Loss: 1.2464265823364258\n",
      "Epoch: 24 | Loss: 1.0078176259994507\n",
      "Epoch: 25 | Loss: 1.0445510149002075\n",
      "Accuracy=0.796875\n",
      "Epoch: 26 | Loss: 0.9072914123535156\n",
      "Epoch: 27 | Loss: 0.9543710350990295\n",
      "Epoch: 28 | Loss: 1.0023105144500732\n",
      "Epoch: 29 | Loss: 0.9167594909667969\n",
      "Epoch: 30 | Loss: 0.9390906691551208\n",
      "Accuracy=0.7578125\n",
      "Epoch: 31 | Loss: 0.9198751449584961\n",
      "Epoch: 32 | Loss: 0.759097695350647\n",
      "Epoch: 33 | Loss: 0.8462948203086853\n",
      "Epoch: 34 | Loss: 0.7721481919288635\n",
      "Epoch: 35 | Loss: 0.8790013194084167\n",
      "Accuracy=0.7734375\n",
      "Epoch: 36 | Loss: 0.7772823572158813\n",
      "Epoch: 37 | Loss: 0.8939689993858337\n",
      "Epoch: 38 | Loss: 0.6735700964927673\n",
      "Epoch: 39 | Loss: 0.8053681254386902\n",
      "Epoch: 40 | Loss: 0.6581649780273438\n",
      "Accuracy=0.859375\n",
      "Epoch: 41 | Loss: 0.6461581587791443\n",
      "Epoch: 42 | Loss: 0.7047203779220581\n",
      "Epoch: 43 | Loss: 0.7170200347900391\n",
      "Epoch: 44 | Loss: 0.6574105024337769\n",
      "Epoch: 45 | Loss: 0.7017344236373901\n",
      "Accuracy=0.828125\n",
      "Epoch: 46 | Loss: 0.6186671853065491\n",
      "Epoch: 47 | Loss: 0.5566362738609314\n",
      "Epoch: 48 | Loss: 0.6217652559280396\n",
      "Epoch: 49 | Loss: 0.5564683079719543\n",
      "Epoch: 50 | Loss: 0.6414505243301392\n",
      "Accuracy=0.828125\n",
      "Epoch: 51 | Loss: 0.6996409893035889\n",
      "Epoch: 52 | Loss: 0.5403065085411072\n",
      "Epoch: 53 | Loss: 0.6494064927101135\n",
      "Epoch: 54 | Loss: 0.5338119864463806\n",
      "Epoch: 55 | Loss: 0.5394256114959717\n",
      "Accuracy=0.859375\n",
      "Epoch: 56 | Loss: 0.6056492924690247\n",
      "Epoch: 57 | Loss: 0.6099039912223816\n",
      "Epoch: 58 | Loss: 0.48717567324638367\n",
      "Epoch: 59 | Loss: 0.4970242381095886\n",
      "Epoch: 60 | Loss: 0.6220759153366089\n",
      "Accuracy=0.8203125\n",
      "Epoch: 61 | Loss: 0.4546250104904175\n",
      "Epoch: 62 | Loss: 0.4691053628921509\n",
      "Epoch: 63 | Loss: 0.4729163944721222\n",
      "Epoch: 64 | Loss: 0.4180715084075928\n",
      "Epoch: 65 | Loss: 0.4812946915626526\n",
      "Accuracy=0.875\n",
      "Epoch: 66 | Loss: 0.624658465385437\n",
      "Epoch: 67 | Loss: 0.6818883419036865\n",
      "Epoch: 68 | Loss: 0.5910193920135498\n",
      "Epoch: 69 | Loss: 0.5038930177688599\n",
      "Epoch: 70 | Loss: 0.5425492525100708\n",
      "Accuracy=0.8359375\n",
      "Epoch: 71 | Loss: 0.48888495564460754\n",
      "Epoch: 72 | Loss: 0.42696237564086914\n",
      "Epoch: 73 | Loss: 0.602139413356781\n",
      "Epoch: 74 | Loss: 0.5610114932060242\n",
      "Epoch: 75 | Loss: 0.5243150591850281\n",
      "Accuracy=0.859375\n",
      "Epoch: 76 | Loss: 0.3784199357032776\n",
      "Epoch: 77 | Loss: 0.5947566032409668\n",
      "Epoch: 78 | Loss: 0.6371034383773804\n",
      "Epoch: 79 | Loss: 0.5653144717216492\n",
      "Epoch: 80 | Loss: 0.6339552998542786\n",
      "Accuracy=0.8125\n",
      "Epoch: 81 | Loss: 0.5105606913566589\n",
      "Epoch: 82 | Loss: 0.4985862374305725\n",
      "Epoch: 83 | Loss: 0.46383947134017944\n",
      "Epoch: 84 | Loss: 0.44512060284614563\n",
      "Epoch: 85 | Loss: 0.5560678243637085\n",
      "Accuracy=0.84375\n",
      "Epoch: 86 | Loss: 0.38306015729904175\n",
      "Epoch: 87 | Loss: 0.42570769786834717\n",
      "Epoch: 88 | Loss: 0.33712038397789\n",
      "Epoch: 89 | Loss: 0.530385434627533\n",
      "Epoch: 90 | Loss: 0.4790480434894562\n",
      "Accuracy=0.8515625\n",
      "Epoch: 91 | Loss: 0.3116995394229889\n",
      "Epoch: 92 | Loss: 0.5460519194602966\n",
      "Epoch: 93 | Loss: 0.41906893253326416\n",
      "Epoch: 94 | Loss: 0.4219508767127991\n",
      "Epoch: 95 | Loss: 0.43576374650001526\n",
      "Accuracy=0.875\n",
      "Epoch: 96 | Loss: 0.4404173791408539\n",
      "Epoch: 97 | Loss: 0.3896213471889496\n",
      "Epoch: 98 | Loss: 0.4105657935142517\n",
      "Epoch: 99 | Loss: 0.4049397110939026\n",
      "Epoch: 100 | Loss: 0.4055565297603607\n",
      "Accuracy=0.921875\n",
      "Epoch: 101 | Loss: 0.31991633772850037\n",
      "Epoch: 102 | Loss: 0.4752618372440338\n",
      "Epoch: 103 | Loss: 0.4362962245941162\n",
      "Epoch: 104 | Loss: 0.3834085464477539\n",
      "Epoch: 105 | Loss: 0.6143389344215393\n",
      "Accuracy=0.8203125\n",
      "Epoch: 106 | Loss: 0.4637426435947418\n",
      "Epoch: 107 | Loss: 0.5334352850914001\n",
      "Epoch: 108 | Loss: 0.41148054599761963\n",
      "Epoch: 109 | Loss: 0.39843663573265076\n",
      "Epoch: 110 | Loss: 0.4724709987640381\n",
      "Accuracy=0.8515625\n",
      "Epoch: 111 | Loss: 0.46382978558540344\n",
      "Epoch: 112 | Loss: 0.4431823194026947\n",
      "Epoch: 113 | Loss: 0.37722450494766235\n",
      "Epoch: 114 | Loss: 0.4243444800376892\n",
      "Epoch: 115 | Loss: 0.6133514642715454\n",
      "Accuracy=0.8046875\n",
      "Epoch: 116 | Loss: 0.5337712168693542\n",
      "Epoch: 117 | Loss: 0.44397205114364624\n",
      "Epoch: 118 | Loss: 0.420640230178833\n",
      "Epoch: 119 | Loss: 0.3993994891643524\n",
      "Epoch: 120 | Loss: 0.3416247069835663\n",
      "Accuracy=0.90625\n",
      "Epoch: 121 | Loss: 0.44920647144317627\n",
      "Epoch: 122 | Loss: 0.43451476097106934\n",
      "Epoch: 123 | Loss: 0.4152032434940338\n",
      "Epoch: 124 | Loss: 0.34562554955482483\n",
      "Epoch: 125 | Loss: 0.3454412817955017\n",
      "Accuracy=0.8984375\n",
      "Epoch: 126 | Loss: 0.4814502000808716\n",
      "Epoch: 127 | Loss: 0.37885385751724243\n",
      "Epoch: 128 | Loss: 0.30451375246047974\n",
      "Epoch: 129 | Loss: 0.5549253821372986\n",
      "Epoch: 130 | Loss: 0.4224737882614136\n",
      "Accuracy=0.90625\n",
      "Epoch: 131 | Loss: 0.4376174509525299\n",
      "Epoch: 132 | Loss: 0.3990538716316223\n",
      "Epoch: 133 | Loss: 0.3360856771469116\n",
      "Epoch: 134 | Loss: 0.4368232190608978\n",
      "Epoch: 135 | Loss: 0.4191087484359741\n",
      "Accuracy=0.8515625\n",
      "Epoch: 136 | Loss: 0.2351446896791458\n",
      "Epoch: 137 | Loss: 0.43285655975341797\n",
      "Epoch: 138 | Loss: 0.44519007205963135\n",
      "Epoch: 139 | Loss: 0.27151036262512207\n",
      "Epoch: 140 | Loss: 0.33811473846435547\n",
      "Accuracy=0.875\n",
      "Epoch: 141 | Loss: 0.4669407904148102\n",
      "Epoch: 142 | Loss: 0.4260164201259613\n",
      "Epoch: 143 | Loss: 0.3858219087123871\n",
      "Epoch: 144 | Loss: 0.3204299211502075\n",
      "Epoch: 145 | Loss: 0.305597186088562\n",
      "Accuracy=0.921875\n",
      "Epoch: 146 | Loss: 0.45875802636146545\n",
      "Epoch: 147 | Loss: 0.3529772460460663\n",
      "Epoch: 148 | Loss: 0.37690213322639465\n",
      "Epoch: 149 | Loss: 0.36742687225341797\n",
      "Epoch: 150 | Loss: 0.41112303733825684\n",
      "Accuracy=0.8671875\n",
      "Epoch: 151 | Loss: 0.2680535912513733\n",
      "Epoch: 152 | Loss: 0.3044542968273163\n",
      "Epoch: 153 | Loss: 0.3722759187221527\n",
      "Epoch: 154 | Loss: 0.4685324430465698\n",
      "Epoch: 155 | Loss: 0.44479531049728394\n",
      "Accuracy=0.859375\n",
      "Epoch: 156 | Loss: 0.3592124879360199\n",
      "Epoch: 157 | Loss: 0.3382842540740967\n",
      "Epoch: 158 | Loss: 0.3235892057418823\n",
      "Epoch: 159 | Loss: 0.3941536545753479\n",
      "Epoch: 160 | Loss: 0.43308278918266296\n",
      "Accuracy=0.8671875\n",
      "Epoch: 161 | Loss: 0.43693894147872925\n",
      "Epoch: 162 | Loss: 0.25077903270721436\n",
      "Epoch: 163 | Loss: 0.47129949927330017\n",
      "Epoch: 164 | Loss: 0.31256064772605896\n",
      "Epoch: 165 | Loss: 0.2511824071407318\n",
      "Accuracy=0.9296875\n",
      "Epoch: 166 | Loss: 0.36026594042778015\n",
      "Epoch: 167 | Loss: 0.2812855839729309\n",
      "Epoch: 168 | Loss: 0.35892367362976074\n",
      "Epoch: 169 | Loss: 0.39471203088760376\n",
      "Epoch: 170 | Loss: 0.44494473934173584\n",
      "Accuracy=0.8515625\n",
      "Epoch: 171 | Loss: 0.40353089570999146\n",
      "Epoch: 172 | Loss: 0.289810448884964\n",
      "Epoch: 173 | Loss: 0.33820563554763794\n",
      "Epoch: 174 | Loss: 0.39526885747909546\n",
      "Epoch: 175 | Loss: 0.4699327349662781\n",
      "Accuracy=0.8671875\n",
      "Epoch: 176 | Loss: 0.46266400814056396\n",
      "Epoch: 177 | Loss: 0.41776737570762634\n",
      "Epoch: 178 | Loss: 0.341981440782547\n",
      "Epoch: 179 | Loss: 0.5880191326141357\n",
      "Epoch: 180 | Loss: 0.2939932942390442\n",
      "Accuracy=0.8984375\n",
      "Epoch: 181 | Loss: 0.2970033288002014\n",
      "Epoch: 182 | Loss: 0.2980479598045349\n",
      "Epoch: 183 | Loss: 0.33714166283607483\n",
      "Epoch: 184 | Loss: 0.3778038024902344\n",
      "Epoch: 185 | Loss: 0.3349255323410034\n",
      "Accuracy=0.9140625\n",
      "Epoch: 186 | Loss: 0.26730284094810486\n",
      "Epoch: 187 | Loss: 0.3235914707183838\n",
      "Epoch: 188 | Loss: 0.2900116443634033\n",
      "Epoch: 189 | Loss: 0.3353436589241028\n",
      "Epoch: 190 | Loss: 0.32051917910575867\n",
      "Accuracy=0.890625\n",
      "Epoch: 191 | Loss: 0.36821040511131287\n",
      "Epoch: 192 | Loss: 0.3494049310684204\n",
      "Epoch: 193 | Loss: 0.3373214304447174\n",
      "Epoch: 194 | Loss: 0.2910082936286926\n",
      "Epoch: 195 | Loss: 0.394978791475296\n",
      "Accuracy=0.8671875\n",
      "Epoch: 196 | Loss: 0.39112550020217896\n",
      "Epoch: 197 | Loss: 0.22533497214317322\n",
      "Epoch: 198 | Loss: 0.2675192654132843\n",
      "Epoch: 199 | Loss: 0.288652628660202\n",
      "Epoch: 200 | Loss: 0.26535511016845703\n",
      "Accuracy=0.9375\n",
      "Epoch: 201 | Loss: 0.37886446714401245\n",
      "Epoch: 202 | Loss: 0.31071826815605164\n",
      "Epoch: 203 | Loss: 0.4142109751701355\n",
      "Epoch: 204 | Loss: 0.40351077914237976\n",
      "Epoch: 205 | Loss: 0.29998573660850525\n",
      "Accuracy=0.90625\n",
      "Epoch: 206 | Loss: 0.38360705971717834\n",
      "Epoch: 207 | Loss: 0.26306188106536865\n",
      "Epoch: 208 | Loss: 0.3074139952659607\n",
      "Epoch: 209 | Loss: 0.2422439008951187\n",
      "Epoch: 210 | Loss: 0.2601344585418701\n",
      "Accuracy=0.9140625\n",
      "Epoch: 211 | Loss: 0.3985545039176941\n",
      "Epoch: 212 | Loss: 0.45830586552619934\n",
      "Epoch: 213 | Loss: 0.2698877155780792\n",
      "Epoch: 214 | Loss: 0.35918423533439636\n",
      "Epoch: 215 | Loss: 0.4862481355667114\n",
      "Accuracy=0.859375\n",
      "Epoch: 216 | Loss: 0.3983072340488434\n",
      "Epoch: 217 | Loss: 0.29113003611564636\n",
      "Epoch: 218 | Loss: 0.34007135033607483\n",
      "Epoch: 219 | Loss: 0.2601333558559418\n",
      "Epoch: 220 | Loss: 0.3683429956436157\n",
      "Accuracy=0.8984375\n",
      "Epoch: 221 | Loss: 0.38704851269721985\n",
      "Epoch: 222 | Loss: 0.28919005393981934\n",
      "Epoch: 223 | Loss: 0.30014973878860474\n",
      "Epoch: 224 | Loss: 0.2953343391418457\n",
      "Epoch: 225 | Loss: 0.3435250222682953\n",
      "Accuracy=0.890625\n",
      "Epoch: 226 | Loss: 0.3796534240245819\n",
      "Epoch: 227 | Loss: 0.28087350726127625\n",
      "Epoch: 228 | Loss: 0.31844303011894226\n",
      "Epoch: 229 | Loss: 0.24763789772987366\n",
      "Epoch: 230 | Loss: 0.28118404746055603\n",
      "Accuracy=0.90625\n",
      "Epoch: 231 | Loss: 0.2845004200935364\n",
      "Epoch: 232 | Loss: 0.3608759343624115\n",
      "Epoch: 233 | Loss: 0.18906088173389435\n",
      "Epoch: 234 | Loss: 0.3691396713256836\n",
      "Epoch: 235 | Loss: 0.3491361141204834\n",
      "Accuracy=0.9140625\n",
      "Epoch: 236 | Loss: 0.30843010544776917\n",
      "Epoch: 237 | Loss: 0.34528160095214844\n",
      "Epoch: 238 | Loss: 0.3358886241912842\n",
      "Epoch: 239 | Loss: 0.4332639276981354\n",
      "Epoch: 240 | Loss: 0.3585996627807617\n",
      "Accuracy=0.8828125\n",
      "Epoch: 241 | Loss: 0.4205779731273651\n",
      "Epoch: 242 | Loss: 0.33312323689460754\n",
      "Epoch: 243 | Loss: 0.3500121235847473\n",
      "Epoch: 244 | Loss: 0.33701449632644653\n",
      "Epoch: 245 | Loss: 0.2832522690296173\n",
      "Accuracy=0.9375\n",
      "Epoch: 246 | Loss: 0.31782612204551697\n",
      "Epoch: 247 | Loss: 0.4458402991294861\n",
      "Epoch: 248 | Loss: 0.36736148595809937\n",
      "Epoch: 249 | Loss: 0.27245840430259705\n",
      "Epoch: 250 | Loss: 0.2792018949985504\n",
      "Accuracy=0.90625\n",
      "Epoch: 251 | Loss: 0.34720513224601746\n",
      "Epoch: 252 | Loss: 0.5533128976821899\n",
      "Epoch: 253 | Loss: 0.26805418729782104\n",
      "Epoch: 254 | Loss: 0.32962849736213684\n",
      "Epoch: 255 | Loss: 0.39445608854293823\n",
      "Accuracy=0.8828125\n",
      "Epoch: 256 | Loss: 0.29727399349212646\n",
      "Epoch: 257 | Loss: 0.270469605922699\n",
      "Epoch: 258 | Loss: 0.29821911454200745\n",
      "Epoch: 259 | Loss: 0.3664054274559021\n",
      "Epoch: 260 | Loss: 0.36332380771636963\n",
      "Accuracy=0.8984375\n",
      "Epoch: 261 | Loss: 0.4603990316390991\n",
      "Epoch: 262 | Loss: 0.5154896974563599\n",
      "Epoch: 263 | Loss: 0.21531283855438232\n",
      "Epoch: 264 | Loss: 0.27834662795066833\n",
      "Epoch: 265 | Loss: 0.4048991799354553\n",
      "Accuracy=0.890625\n",
      "Epoch: 266 | Loss: 0.2370661348104477\n",
      "Epoch: 267 | Loss: 0.402208536863327\n",
      "Epoch: 268 | Loss: 0.3115037679672241\n",
      "Epoch: 269 | Loss: 0.28750506043434143\n",
      "Epoch: 270 | Loss: 0.41973671317100525\n",
      "Accuracy=0.890625\n",
      "Epoch: 271 | Loss: 0.3442681133747101\n",
      "Epoch: 272 | Loss: 0.32164013385772705\n",
      "Epoch: 273 | Loss: 0.3607236444950104\n",
      "Epoch: 274 | Loss: 0.15337853133678436\n",
      "Epoch: 275 | Loss: 0.30952638387680054\n",
      "Accuracy=0.9140625\n",
      "Epoch: 276 | Loss: 0.3942874073982239\n",
      "Epoch: 277 | Loss: 0.21835650503635406\n",
      "Epoch: 278 | Loss: 0.4800463914871216\n",
      "Epoch: 279 | Loss: 0.332572340965271\n",
      "Epoch: 280 | Loss: 0.37101060152053833\n",
      "Accuracy=0.90625\n",
      "Epoch: 281 | Loss: 0.27111223340034485\n",
      "Epoch: 282 | Loss: 0.39632439613342285\n",
      "Epoch: 283 | Loss: 0.341816246509552\n",
      "Epoch: 284 | Loss: 0.3170873820781708\n",
      "Epoch: 285 | Loss: 0.2129620462656021\n",
      "Accuracy=0.9375\n",
      "Epoch: 286 | Loss: 0.2923750579357147\n",
      "Epoch: 287 | Loss: 0.2935672998428345\n",
      "Epoch: 288 | Loss: 0.3373410701751709\n",
      "Epoch: 289 | Loss: 0.4421442151069641\n",
      "Epoch: 290 | Loss: 0.37358275055885315\n",
      "Accuracy=0.90625\n",
      "Epoch: 291 | Loss: 0.27255314588546753\n",
      "Epoch: 292 | Loss: 0.2734217941761017\n",
      "Epoch: 293 | Loss: 0.3147056996822357\n",
      "Epoch: 294 | Loss: 0.21702735126018524\n",
      "Epoch: 295 | Loss: 0.2657627463340759\n",
      "Accuracy=0.9296875\n",
      "Epoch: 296 | Loss: 0.29895174503326416\n",
      "Epoch: 297 | Loss: 0.40795791149139404\n",
      "Epoch: 298 | Loss: 0.2625270187854767\n",
      "Epoch: 299 | Loss: 0.34463736414909363\n",
      "Epoch: 300 | Loss: 0.36977988481521606\n",
      "Accuracy=0.890625\n",
      "Epoch: 301 | Loss: 0.40940770506858826\n",
      "Epoch: 302 | Loss: 0.17454291880130768\n",
      "Epoch: 303 | Loss: 0.18730977177619934\n",
      "Epoch: 304 | Loss: 0.32619670033454895\n",
      "Epoch: 305 | Loss: 0.23773224651813507\n",
      "Accuracy=0.953125\n",
      "Epoch: 306 | Loss: 0.31375351548194885\n",
      "Epoch: 307 | Loss: 0.29893940687179565\n",
      "Epoch: 308 | Loss: 0.32237139344215393\n",
      "Epoch: 309 | Loss: 0.2528861463069916\n",
      "Epoch: 310 | Loss: 0.2288156896829605\n",
      "Accuracy=0.921875\n",
      "Epoch: 311 | Loss: 0.25111332535743713\n",
      "Epoch: 312 | Loss: 0.3053930401802063\n",
      "Epoch: 313 | Loss: 0.5050140023231506\n",
      "Epoch: 314 | Loss: 0.23169733583927155\n",
      "Epoch: 315 | Loss: 0.2457774430513382\n",
      "Accuracy=0.9375\n",
      "Epoch: 316 | Loss: 0.26218271255493164\n",
      "Epoch: 317 | Loss: 0.21730713546276093\n",
      "Epoch: 318 | Loss: 0.42934542894363403\n",
      "Epoch: 319 | Loss: 0.3208586573600769\n",
      "Epoch: 320 | Loss: 0.2624582052230835\n",
      "Accuracy=0.921875\n",
      "Epoch: 321 | Loss: 0.22395358979701996\n",
      "Epoch: 322 | Loss: 0.22750642895698547\n",
      "Epoch: 323 | Loss: 0.2619147002696991\n",
      "Epoch: 324 | Loss: 0.2503587305545807\n",
      "Epoch: 325 | Loss: 0.33938828110694885\n",
      "Accuracy=0.90625\n",
      "Epoch: 326 | Loss: 0.2648596167564392\n",
      "Epoch: 327 | Loss: 0.43188658356666565\n",
      "Epoch: 328 | Loss: 0.281500905752182\n",
      "Epoch: 329 | Loss: 0.2956676483154297\n",
      "Epoch: 330 | Loss: 0.36288562417030334\n",
      "Accuracy=0.8671875\n",
      "Epoch: 331 | Loss: 0.30544158816337585\n",
      "Epoch: 332 | Loss: 0.26566627621650696\n",
      "Epoch: 333 | Loss: 0.2950657308101654\n",
      "Epoch: 334 | Loss: 0.3957425057888031\n",
      "Epoch: 335 | Loss: 0.3433268666267395\n",
      "Accuracy=0.8828125\n",
      "Epoch: 336 | Loss: 0.318778395652771\n",
      "Epoch: 337 | Loss: 0.28048211336135864\n",
      "Epoch: 338 | Loss: 0.3096220791339874\n",
      "Epoch: 339 | Loss: 0.30234652757644653\n",
      "Epoch: 340 | Loss: 0.23415352404117584\n",
      "Accuracy=0.9453125\n",
      "Epoch: 341 | Loss: 0.1586928367614746\n",
      "Epoch: 342 | Loss: 0.2372022569179535\n",
      "Epoch: 343 | Loss: 0.26375871896743774\n",
      "Epoch: 344 | Loss: 0.29298174381256104\n",
      "Epoch: 345 | Loss: 0.2906097173690796\n",
      "Accuracy=0.921875\n",
      "Epoch: 346 | Loss: 0.2034498006105423\n",
      "Epoch: 347 | Loss: 0.2962726056575775\n",
      "Epoch: 348 | Loss: 0.29944440722465515\n",
      "Epoch: 349 | Loss: 0.20602460205554962\n",
      "Epoch: 350 | Loss: 0.5067786574363708\n",
      "Accuracy=0.8359375\n",
      "Epoch: 351 | Loss: 0.2503211498260498\n",
      "Epoch: 352 | Loss: 0.2425869107246399\n",
      "Epoch: 353 | Loss: 0.29365572333335876\n",
      "Epoch: 354 | Loss: 0.2794225215911865\n",
      "Epoch: 355 | Loss: 0.40994924306869507\n",
      "Accuracy=0.890625\n",
      "Epoch: 356 | Loss: 0.2584404945373535\n",
      "Epoch: 357 | Loss: 0.29630210995674133\n",
      "Epoch: 358 | Loss: 0.4225750267505646\n",
      "Epoch: 359 | Loss: 0.22020003199577332\n",
      "Epoch: 360 | Loss: 0.42985814809799194\n",
      "Accuracy=0.875\n",
      "Epoch: 361 | Loss: 0.4651031494140625\n",
      "Epoch: 362 | Loss: 0.28287002444267273\n",
      "Epoch: 363 | Loss: 0.2632247805595398\n",
      "Epoch: 364 | Loss: 0.33772432804107666\n",
      "Epoch: 365 | Loss: 0.2334757149219513\n",
      "Accuracy=0.9296875\n",
      "Epoch: 366 | Loss: 0.22241833806037903\n",
      "Epoch: 367 | Loss: 0.2644537091255188\n",
      "Epoch: 368 | Loss: 0.2211979627609253\n",
      "Epoch: 369 | Loss: 0.23755711317062378\n",
      "Epoch: 370 | Loss: 0.3069719970226288\n",
      "Accuracy=0.921875\n",
      "Epoch: 371 | Loss: 0.26720306277275085\n",
      "Epoch: 372 | Loss: 0.1625393033027649\n",
      "Epoch: 373 | Loss: 0.3538858890533447\n",
      "Epoch: 374 | Loss: 0.3211466073989868\n",
      "Epoch: 375 | Loss: 0.22683532536029816\n",
      "Accuracy=0.953125\n",
      "Epoch: 376 | Loss: 0.2967260479927063\n",
      "Epoch: 377 | Loss: 0.1944236010313034\n",
      "Epoch: 378 | Loss: 0.20676271617412567\n",
      "Epoch: 379 | Loss: 0.16413071751594543\n",
      "Epoch: 380 | Loss: 0.3045627474784851\n",
      "Accuracy=0.921875\n",
      "Epoch: 381 | Loss: 0.29347771406173706\n",
      "Epoch: 382 | Loss: 0.2813240587711334\n",
      "Epoch: 383 | Loss: 0.36302998661994934\n",
      "Epoch: 384 | Loss: 0.17755399644374847\n",
      "Epoch: 385 | Loss: 0.20370995998382568\n",
      "Accuracy=0.953125\n",
      "Epoch: 386 | Loss: 0.27094200253486633\n",
      "Epoch: 387 | Loss: 0.30859360098838806\n",
      "Epoch: 388 | Loss: 0.27262553572654724\n",
      "Epoch: 389 | Loss: 0.2904035747051239\n",
      "Epoch: 390 | Loss: 0.3475952744483948\n",
      "Accuracy=0.9140625\n",
      "Epoch: 391 | Loss: 0.2787761986255646\n",
      "Epoch: 392 | Loss: 0.3240731656551361\n",
      "Epoch: 393 | Loss: 0.22815023362636566\n",
      "Epoch: 394 | Loss: 0.2687218487262726\n",
      "Epoch: 395 | Loss: 0.3322119414806366\n",
      "Accuracy=0.9375\n",
      "Epoch: 396 | Loss: 0.33194753527641296\n",
      "Epoch: 397 | Loss: 0.18228374421596527\n",
      "Epoch: 398 | Loss: 0.3898480236530304\n",
      "Epoch: 399 | Loss: 0.3052040636539459\n",
      "Epoch: 400 | Loss: 0.37261050939559937\n",
      "Accuracy=0.90625\n",
      "Epoch: 401 | Loss: 0.14574548602104187\n",
      "Epoch: 402 | Loss: 0.29627618193626404\n",
      "Epoch: 403 | Loss: 0.36230161786079407\n",
      "Epoch: 404 | Loss: 0.24204733967781067\n",
      "Epoch: 405 | Loss: 0.31698963046073914\n",
      "Accuracy=0.921875\n",
      "Epoch: 406 | Loss: 0.2538810074329376\n",
      "Epoch: 407 | Loss: 0.2524208724498749\n",
      "Epoch: 408 | Loss: 0.2810424864292145\n",
      "Epoch: 409 | Loss: 0.237814262509346\n",
      "Epoch: 410 | Loss: 0.2750432789325714\n",
      "Accuracy=0.921875\n",
      "Epoch: 411 | Loss: 0.2179274708032608\n",
      "Epoch: 412 | Loss: 0.22748753428459167\n",
      "Epoch: 413 | Loss: 0.2774037718772888\n",
      "Epoch: 414 | Loss: 0.2896397113800049\n",
      "Epoch: 415 | Loss: 0.2693890333175659\n",
      "Accuracy=0.921875\n",
      "Epoch: 416 | Loss: 0.22082693874835968\n",
      "Epoch: 417 | Loss: 0.30492833256721497\n",
      "Epoch: 418 | Loss: 0.20713768899440765\n",
      "Epoch: 419 | Loss: 0.3223259150981903\n",
      "Epoch: 420 | Loss: 0.19389599561691284\n",
      "Accuracy=0.9453125\n",
      "Epoch: 421 | Loss: 0.2631864547729492\n",
      "Epoch: 422 | Loss: 0.24467450380325317\n",
      "Epoch: 423 | Loss: 0.2830059230327606\n",
      "Epoch: 424 | Loss: 0.3478807806968689\n",
      "Epoch: 425 | Loss: 0.4468120038509369\n",
      "Accuracy=0.890625\n",
      "Epoch: 426 | Loss: 0.25439131259918213\n",
      "Epoch: 427 | Loss: 0.18161356449127197\n",
      "Epoch: 428 | Loss: 0.2673768401145935\n",
      "Epoch: 429 | Loss: 0.3493156135082245\n",
      "Epoch: 430 | Loss: 0.35502174496650696\n",
      "Accuracy=0.890625\n",
      "Epoch: 431 | Loss: 0.23298291862010956\n",
      "Epoch: 432 | Loss: 0.2640259563922882\n",
      "Epoch: 433 | Loss: 0.1770138293504715\n",
      "Epoch: 434 | Loss: 0.30878621339797974\n",
      "Epoch: 435 | Loss: 0.223859041929245\n",
      "Accuracy=0.9375\n",
      "Epoch: 436 | Loss: 0.28584370017051697\n",
      "Epoch: 437 | Loss: 0.3632104694843292\n",
      "Epoch: 438 | Loss: 0.19461318850517273\n",
      "Epoch: 439 | Loss: 0.20379123091697693\n",
      "Epoch: 440 | Loss: 0.21542929112911224\n",
      "Accuracy=0.9453125\n",
      "Epoch: 441 | Loss: 0.22903500497341156\n",
      "Epoch: 442 | Loss: 0.22518889605998993\n",
      "Epoch: 443 | Loss: 0.26963576674461365\n",
      "Epoch: 444 | Loss: 0.15374626219272614\n",
      "Epoch: 445 | Loss: 0.26037025451660156\n",
      "Accuracy=0.9453125\n",
      "Epoch: 446 | Loss: 0.21948561072349548\n",
      "Epoch: 447 | Loss: 0.1938345730304718\n",
      "Epoch: 448 | Loss: 0.1997673064470291\n",
      "Epoch: 449 | Loss: 0.3211660385131836\n",
      "Epoch: 450 | Loss: 0.2567344903945923\n",
      "Accuracy=0.9375\n",
      "Epoch: 451 | Loss: 0.2158946543931961\n",
      "Epoch: 452 | Loss: 0.2644977867603302\n",
      "Epoch: 453 | Loss: 0.2320384830236435\n",
      "Epoch: 454 | Loss: 0.4845156967639923\n",
      "Epoch: 455 | Loss: 0.2859002649784088\n",
      "Accuracy=0.9140625\n",
      "Epoch: 456 | Loss: 0.1921025812625885\n",
      "Epoch: 457 | Loss: 0.2960250675678253\n",
      "Epoch: 458 | Loss: 0.22704832255840302\n",
      "Epoch: 459 | Loss: 0.17656366527080536\n",
      "Epoch: 460 | Loss: 0.31467533111572266\n",
      "Accuracy=0.90625\n",
      "Epoch: 461 | Loss: 0.4156481623649597\n",
      "Epoch: 462 | Loss: 0.2589840292930603\n",
      "Epoch: 463 | Loss: 0.4854739308357239\n",
      "Epoch: 464 | Loss: 0.2521326243877411\n",
      "Epoch: 465 | Loss: 0.1737968623638153\n",
      "Accuracy=0.96875\n",
      "Epoch: 466 | Loss: 0.2605566680431366\n",
      "Epoch: 467 | Loss: 0.30701330304145813\n",
      "Epoch: 468 | Loss: 0.16176167130470276\n",
      "Epoch: 469 | Loss: 0.3368417024612427\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "\n",
    "optim = torch.optim.SGD(params=mlp.parameters(), lr=0.2)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for x, y in train_loader:\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    y_pred = mlp.forward(x)\n",
    "    \n",
    "    loss = loss_func(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "    print(f\"Epoch: {epoch} | Loss: {loss.item()}\")\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        labels = y_pred.argmax(dim=1)\n",
    "        print(f\"Accuracy={(labels == y).float().mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
